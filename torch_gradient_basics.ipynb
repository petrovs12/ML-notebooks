{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= torch.tensor([1,2,3,4,5,6,7,8,9,10],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.], device='mps:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.tensor([1,2,3,4,5],dtype=torch.float32,requires_grad=True)\n",
    "y=torch.sum(x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.,  4.,  6.,  8., 10.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 9.5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m---> 23\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_prime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 16\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(x, weights_prime, optimizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39mmean((y_pred \u001b[38;5;241m-\u001b[39m y) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# RMSE\u001b[39;00m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# loss.backward()\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ml-tutorials/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ml-tutorials/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ml-tutorials/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# implement gradient descent in torch\n",
    "from torch import ones_like\n",
    "\n",
    "\n",
    "x= torch.tensor([1,2,3,4,5],dtype=torch.float32,requires_grad=True)\n",
    "weights = torch.tensor([0.1,0.2,0.3,0.4,0.5],dtype=torch.float32,requires_grad=True)\n",
    "y=x@weights.T\n",
    "weights_prime= ones_like(weights,requires_grad=True)\n",
    "optimizer = optim.SGD([weights_prime], lr=0.01,)\n",
    "\n",
    "def train_step(x, weights_prime, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = x @ weights_prime.T\n",
    "    loss = torch.sqrt(torch.mean((y_pred - y) ** 2))  # RMSE\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # loss.backward()\n",
    "    return loss.item()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    loss = train_step(x, weights_prime, optimizer)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(5.5000, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Epoch 0, Loss: 5.5\n",
      "tensor([0.0010, 0.0020, 0.0030, 0.0040, 0.0050], requires_grad=True)\n",
      "loss=tensor(5.4450, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0010, 0.0020, 0.0030, 0.0040, 0.0050], requires_grad=True)\n",
      "loss=tensor(5.3900, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1.0000, -2.0000, -3.0000, -4.0000, -5.0000])\n",
      "weights_prime=tensor([0.0020, 0.0040, 0.0060, 0.0080, 0.0100], requires_grad=True)\n",
      "loss=tensor(5.3350, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0030, 0.0060, 0.0090, 0.0120, 0.0150], requires_grad=True)\n",
      "loss=tensor(5.2800, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0040, 0.0080, 0.0120, 0.0160, 0.0200], requires_grad=True)\n",
      "loss=tensor(5.2250, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0050, 0.0100, 0.0150, 0.0200, 0.0250], requires_grad=True)\n",
      "loss=tensor(5.1700, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0060, 0.0120, 0.0180, 0.0240, 0.0300], requires_grad=True)\n",
      "loss=tensor(5.1150, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0070, 0.0140, 0.0210, 0.0280, 0.0350], requires_grad=True)\n",
      "loss=tensor(5.0600, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1.0000, -2.0000, -3.0000, -4.0000, -5.0000])\n",
      "weights_prime=tensor([0.0080, 0.0160, 0.0240, 0.0320, 0.0400], requires_grad=True)\n",
      "loss=tensor(5.0050, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0090, 0.0180, 0.0270, 0.0360, 0.0450], requires_grad=True)\n",
      "loss=tensor(4.9500, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0100, 0.0200, 0.0300, 0.0400, 0.0500], requires_grad=True)\n",
      "loss=tensor(4.8950, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0110, 0.0220, 0.0330, 0.0440, 0.0550], requires_grad=True)\n",
      "loss=tensor(4.8400, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0120, 0.0240, 0.0360, 0.0480, 0.0600], requires_grad=True)\n",
      "loss=tensor(4.7850, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0130, 0.0260, 0.0390, 0.0520, 0.0650], requires_grad=True)\n",
      "loss=tensor(4.7300, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1.0000, -2.0000, -3.0000, -4.0000, -5.0000])\n",
      "weights_prime=tensor([0.0140, 0.0280, 0.0420, 0.0560, 0.0700], requires_grad=True)\n",
      "loss=tensor(4.6750, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0150, 0.0300, 0.0450, 0.0600, 0.0750], requires_grad=True)\n",
      "loss=tensor(4.6200, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1.0000, -2.0000, -3.0000, -4.0000, -5.0000])\n",
      "weights_prime=tensor([0.0160, 0.0320, 0.0480, 0.0640, 0.0800], requires_grad=True)\n",
      "loss=tensor(4.5650, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0170, 0.0340, 0.0510, 0.0680, 0.0850], requires_grad=True)\n",
      "loss=tensor(4.5100, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0180, 0.0360, 0.0540, 0.0720, 0.0900], requires_grad=True)\n",
      "loss=tensor(4.4550, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0190, 0.0380, 0.0570, 0.0760, 0.0950], requires_grad=True)\n",
      "loss=tensor(4.4000, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0200, 0.0400, 0.0600, 0.0800, 0.1000], requires_grad=True)\n",
      "loss=tensor(4.3450, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0210, 0.0420, 0.0630, 0.0840, 0.1050], requires_grad=True)\n",
      "loss=tensor(4.2900, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0220, 0.0440, 0.0660, 0.0880, 0.1100], requires_grad=True)\n",
      "loss=tensor(4.2350, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0230, 0.0460, 0.0690, 0.0920, 0.1150], requires_grad=True)\n",
      "loss=tensor(4.1800, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0240, 0.0480, 0.0720, 0.0960, 0.1200], requires_grad=True)\n",
      "loss=tensor(4.1250, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0250, 0.0500, 0.0750, 0.1000, 0.1250], requires_grad=True)\n",
      "loss=tensor(4.0700, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0260, 0.0520, 0.0780, 0.1040, 0.1300], requires_grad=True)\n",
      "loss=tensor(4.0150, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0270, 0.0540, 0.0810, 0.1080, 0.1350], requires_grad=True)\n",
      "loss=tensor(3.9600, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1.0000, -2.0000, -3.0000, -4.0000, -5.0000])\n",
      "weights_prime=tensor([0.0280, 0.0560, 0.0840, 0.1120, 0.1400], requires_grad=True)\n",
      "loss=tensor(3.9050, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1.0000, -2.0000, -3.0000, -4.0000, -5.0000])\n",
      "weights_prime=tensor([0.0290, 0.0580, 0.0870, 0.1160, 0.1450], requires_grad=True)\n",
      "loss=tensor(3.8500, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0300, 0.0600, 0.0900, 0.1200, 0.1500], requires_grad=True)\n",
      "loss=tensor(3.7950, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1.0000, -2.0000, -3.0000, -4.0000, -5.0000])\n",
      "weights_prime=tensor([0.0310, 0.0620, 0.0930, 0.1240, 0.1550], requires_grad=True)\n",
      "loss=tensor(3.7400, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0320, 0.0640, 0.0960, 0.1280, 0.1600], requires_grad=True)\n",
      "loss=tensor(3.6850, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0330, 0.0660, 0.0990, 0.1320, 0.1650], requires_grad=True)\n",
      "loss=tensor(3.6300, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0340, 0.0680, 0.1020, 0.1360, 0.1700], requires_grad=True)\n",
      "loss=tensor(3.5750, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1.0000, -2.0000, -3.0000, -4.0000, -5.0000])\n",
      "weights_prime=tensor([0.0350, 0.0700, 0.1050, 0.1400, 0.1750], requires_grad=True)\n",
      "loss=tensor(3.5200, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0360, 0.0720, 0.1080, 0.1440, 0.1800], requires_grad=True)\n",
      "loss=tensor(3.4650, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0370, 0.0740, 0.1110, 0.1480, 0.1850], requires_grad=True)\n",
      "loss=tensor(3.4100, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1.0000, -2.0000, -3.0000, -4.0000, -5.0000])\n",
      "weights_prime=tensor([0.0380, 0.0760, 0.1140, 0.1520, 0.1900], requires_grad=True)\n",
      "loss=tensor(3.3550, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0390, 0.0780, 0.1170, 0.1560, 0.1950], requires_grad=True)\n",
      "loss=tensor(3.3000, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0400, 0.0800, 0.1200, 0.1600, 0.2000], requires_grad=True)\n",
      "loss=tensor(3.2450, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0410, 0.0820, 0.1230, 0.1640, 0.2050], requires_grad=True)\n",
      "loss=tensor(3.1900, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0420, 0.0840, 0.1260, 0.1680, 0.2100], requires_grad=True)\n",
      "loss=tensor(3.1350, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0430, 0.0860, 0.1290, 0.1720, 0.2150], requires_grad=True)\n",
      "loss=tensor(3.0800, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0440, 0.0880, 0.1320, 0.1760, 0.2200], requires_grad=True)\n",
      "loss=tensor(3.0250, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0450, 0.0900, 0.1350, 0.1800, 0.2250], requires_grad=True)\n",
      "loss=tensor(2.9700, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0460, 0.0920, 0.1380, 0.1840, 0.2300], requires_grad=True)\n",
      "loss=tensor(2.9150, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0470, 0.0940, 0.1410, 0.1880, 0.2350], requires_grad=True)\n",
      "loss=tensor(2.8600, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1.0000, -2.0000, -3.0000, -4.0000, -5.0000])\n",
      "weights_prime=tensor([0.0480, 0.0960, 0.1440, 0.1920, 0.2400], requires_grad=True)\n",
      "loss=tensor(2.8050, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0490, 0.0980, 0.1470, 0.1960, 0.2450], requires_grad=True)\n",
      "loss=tensor(2.7500, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1.0000, -2.0000, -3.0000, -4.0000, -5.0000])\n",
      "weights_prime=tensor([0.0500, 0.1000, 0.1500, 0.2000, 0.2500], requires_grad=True)\n",
      "loss=tensor(2.6950, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0510, 0.1020, 0.1530, 0.2040, 0.2550], requires_grad=True)\n",
      "loss=tensor(2.6400, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0520, 0.1040, 0.1560, 0.2080, 0.2600], requires_grad=True)\n",
      "loss=tensor(2.5850, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0530, 0.1060, 0.1590, 0.2120, 0.2650], requires_grad=True)\n",
      "loss=tensor(2.5300, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0540, 0.1080, 0.1620, 0.2160, 0.2700], requires_grad=True)\n",
      "loss=tensor(2.4750, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0550, 0.1100, 0.1650, 0.2200, 0.2750], requires_grad=True)\n",
      "loss=tensor(2.4200, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0560, 0.1120, 0.1680, 0.2240, 0.2800], requires_grad=True)\n",
      "loss=tensor(2.3650, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0570, 0.1140, 0.1710, 0.2280, 0.2850], requires_grad=True)\n",
      "loss=tensor(2.3100, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0580, 0.1160, 0.1740, 0.2320, 0.2900], requires_grad=True)\n",
      "loss=tensor(2.2550, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0590, 0.1180, 0.1770, 0.2360, 0.2950], requires_grad=True)\n",
      "loss=tensor(2.2000, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0600, 0.1200, 0.1800, 0.2400, 0.3000], requires_grad=True)\n",
      "loss=tensor(2.1450, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0610, 0.1220, 0.1830, 0.2440, 0.3050], requires_grad=True)\n",
      "loss=tensor(2.0900, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0620, 0.1240, 0.1860, 0.2480, 0.3100], requires_grad=True)\n",
      "loss=tensor(2.0350, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0630, 0.1260, 0.1890, 0.2520, 0.3150], requires_grad=True)\n",
      "loss=tensor(1.9800, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0640, 0.1280, 0.1920, 0.2560, 0.3200], requires_grad=True)\n",
      "loss=tensor(1.9250, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0650, 0.1300, 0.1950, 0.2600, 0.3250], requires_grad=True)\n",
      "loss=tensor(1.8700, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0660, 0.1320, 0.1980, 0.2640, 0.3300], requires_grad=True)\n",
      "loss=tensor(1.8150, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0670, 0.1340, 0.2010, 0.2680, 0.3350], requires_grad=True)\n",
      "loss=tensor(1.7600, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1.0000, -2.0000, -3.0000, -4.0000, -5.0000])\n",
      "weights_prime=tensor([0.0680, 0.1360, 0.2040, 0.2720, 0.3400], requires_grad=True)\n",
      "loss=tensor(1.7050, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0690, 0.1380, 0.2070, 0.2760, 0.3450], requires_grad=True)\n",
      "loss=tensor(1.6500, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0700, 0.1400, 0.2100, 0.2800, 0.3500], requires_grad=True)\n",
      "loss=tensor(1.5950, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0710, 0.1420, 0.2130, 0.2840, 0.3550], requires_grad=True)\n",
      "loss=tensor(1.5400, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0720, 0.1440, 0.2160, 0.2880, 0.3600], requires_grad=True)\n",
      "loss=tensor(1.4850, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0730, 0.1460, 0.2190, 0.2920, 0.3650], requires_grad=True)\n",
      "loss=tensor(1.4300, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0740, 0.1480, 0.2220, 0.2960, 0.3700], requires_grad=True)\n",
      "loss=tensor(1.3750, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0750, 0.1500, 0.2250, 0.3000, 0.3750], requires_grad=True)\n",
      "loss=tensor(1.3200, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0760, 0.1520, 0.2280, 0.3040, 0.3800], requires_grad=True)\n",
      "loss=tensor(1.2650, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0770, 0.1540, 0.2310, 0.3080, 0.3850], requires_grad=True)\n",
      "loss=tensor(1.2100, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0780, 0.1560, 0.2340, 0.3120, 0.3900], requires_grad=True)\n",
      "loss=tensor(1.1550, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0790, 0.1580, 0.2370, 0.3160, 0.3950], requires_grad=True)\n",
      "loss=tensor(1.1000, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0800, 0.1600, 0.2400, 0.3200, 0.4000], requires_grad=True)\n",
      "loss=tensor(1.0450, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0810, 0.1620, 0.2430, 0.3240, 0.4050], requires_grad=True)\n",
      "loss=tensor(0.9900, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0820, 0.1640, 0.2460, 0.3280, 0.4100], requires_grad=True)\n",
      "loss=tensor(0.9350, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0830, 0.1660, 0.2490, 0.3320, 0.4150], requires_grad=True)\n",
      "loss=tensor(0.8800, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0840, 0.1680, 0.2520, 0.3360, 0.4200], requires_grad=True)\n",
      "loss=tensor(0.8250, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0850, 0.1700, 0.2550, 0.3400, 0.4250], requires_grad=True)\n",
      "loss=tensor(0.7700, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0860, 0.1720, 0.2580, 0.3440, 0.4300], requires_grad=True)\n",
      "loss=tensor(0.7150, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0870, 0.1740, 0.2610, 0.3480, 0.4350], requires_grad=True)\n",
      "loss=tensor(0.6600, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0880, 0.1760, 0.2640, 0.3520, 0.4400], requires_grad=True)\n",
      "loss=tensor(0.6050, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0890, 0.1780, 0.2670, 0.3560, 0.4450], requires_grad=True)\n",
      "loss=tensor(0.5500, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0900, 0.1800, 0.2700, 0.3600, 0.4500], requires_grad=True)\n",
      "loss=tensor(0.4950, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0910, 0.1820, 0.2730, 0.3640, 0.4550], requires_grad=True)\n",
      "loss=tensor(0.4400, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0920, 0.1840, 0.2760, 0.3680, 0.4600], requires_grad=True)\n",
      "loss=tensor(0.3850, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0930, 0.1860, 0.2790, 0.3720, 0.4650], requires_grad=True)\n",
      "loss=tensor(0.3300, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0940, 0.1880, 0.2820, 0.3760, 0.4700], requires_grad=True)\n",
      "loss=tensor(0.2750, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0950, 0.1900, 0.2850, 0.3800, 0.4750], requires_grad=True)\n",
      "loss=tensor(0.2200, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0960, 0.1920, 0.2880, 0.3840, 0.4800], requires_grad=True)\n",
      "loss=tensor(0.1650, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0970, 0.1940, 0.2910, 0.3880, 0.4850], requires_grad=True)\n",
      "loss=tensor(0.1100, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0980, 0.1960, 0.2940, 0.3920, 0.4900], requires_grad=True)\n",
      "loss=tensor(0.0550, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.0990, 0.1980, 0.2970, 0.3960, 0.4950], requires_grad=True)\n",
      "loss=tensor(9.5367e-07, grad_fn=<SqrtBackward0>)\n",
      "weights_prime.grad=tensor([-1., -2., -3., -4., -5.])\n",
      "weights_prime=tensor([0.1000, 0.2000, 0.3000, 0.4000, 0.5000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Make x and weights non-trainable for generating y\n",
    "x = torch.tensor([1,2,3,4,5], dtype=torch.float32)\n",
    "weights = torch.tensor([0.1,0.2,0.3,0.4,0.5], dtype=torch.float32)\n",
    "y = (x @ weights.T).detach()\n",
    "\n",
    "weights_prime = torch.zeros_like(weights, requires_grad=True)\n",
    "optimizer = optim.SGD([weights_prime], lr=0.001)\n",
    "\n",
    "def train_step(x, weights_prime, optimizer, y):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = x @ weights_prime.T\n",
    "    loss = torch.sqrt(((y_pred - y) ** 2))\n",
    "    print(f\"{loss=}\")\n",
    "    loss.backward()\n",
    "    print(f\"{weights_prime.grad=}\")\n",
    "    print(f\"{weights_prime=}\")\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "for epoch in range(1000): \n",
    "    loss = train_step(x, weights_prime, optimizer, y)\n",
    "    if loss < 1e-5:\n",
    "        break\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss}')\n",
    "        print(weights_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.2000, 0.3000, 0.4000, 0.5000])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
