{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy, mutual information, kl divergence, cross entropy, conditional entropy\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n"
     ]
    }
   ],
   "source": [
    "# entropy definition based on a value-probability dictionary\n",
    "def entropy(p:Dict[str, float]) -> float:\n",
    "    return -sum([p[i]*math.log2(p[i]) for i in p])\n",
    "\n",
    "example_alphabet_probs={'a':0.5, 'b':0.25, 'c':0.25}\n",
    "print(entropy(example_alphabet_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of the first dictionary: 2.968237770572419\n",
      "Entropy of the second dictionary: 3.3371706097649607\n",
      "Mutual Information: 0.607478068353195\n",
      "Optimal Coding Scheme for the first dictionary: {'w': '00', 'l': '0001', 'e': '0010', 'a': '0011', 's': '0100', 'i': '0101', 'f': '0110', 'u': '0111', 'm': '1000', 'q': '01001', 'o': '01010', 'j': '01011', 'x': '001100', 't': '001101', 'n': '001110', 'r': '001111', 'z': '010000', 'h': '0010001', 'g': '0010010', 'p': '0010011', 'c': '0010100', 'd': '00010101', 'y': '00010110', 'k': '000010111', 'v': '000011000', 'b': '00000011001'}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities do not sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(p\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(symbols, size\u001b[38;5;241m=\u001b[39mn, p\u001b[38;5;241m=\u001b[39mprobabilities))\n\u001b[0;32m---> 40\u001b[0m generated_values \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_probs_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated values from the second dictionary: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_values[:\u001b[38;5;241m50\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Demonstrate the difference in coding schemes\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m, in \u001b[0;36mgenerate_values\u001b[0;34m(p, n)\u001b[0m\n\u001b[1;32m     36\u001b[0m symbols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(p\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     37\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(p\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprobabilities\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:975\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities do not sum to 1"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "# Define two example dictionaries with the same alphabet but different probabilities\n",
    "example_probs_1 = {chr(97 + i): np.random.dirichlet(np.ones(26), size=1)[0][i] for i in range(26)}\n",
    "example_probs_2 = {chr(97 + i): np.random.dirichlet(np.ones(26), size=1)[0][i] for i in range(26)}\n",
    "\n",
    "# Calculate the entropy of the first dictionary\n",
    "entropy_1 = entropy(example_probs_1)\n",
    "print(f\"Entropy of the first dictionary: {entropy_1}\")\n",
    "\n",
    "# Calculate the entropy of the second dictionary\n",
    "entropy_2 = entropy(example_probs_2)\n",
    "print(f\"Entropy of the second dictionary: {entropy_2}\")\n",
    "\n",
    "# Calculate the mutual information\n",
    "def mutual_information(p: Dict[str, float], q: Dict[str, float]) -> float:\n",
    "    return sum([p[i] * math.log2(p[i] / q[i]) for i in p])\n",
    "\n",
    "mutual_info = mutual_information(example_probs_1, example_probs_2)\n",
    "print(f\"Mutual Information: {mutual_info}\")\n",
    "\n",
    "# Create an optimal coding scheme for the first dictionary\n",
    "def optimal_coding_scheme(p: Dict[str, float]) -> Dict[str, str]:\n",
    "    sorted_probs = sorted(p.items(), key=lambda item: item[1], reverse=True)\n",
    "    coding_scheme = {}\n",
    "    code = 0\n",
    "    for symbol, prob in sorted_probs:\n",
    "        coding_scheme[symbol] = format(code, 'b').zfill(int(-math.log2(prob)))\n",
    "        code += 1\n",
    "    return coding_scheme\n",
    "\n",
    "coding_scheme_1 = optimal_coding_scheme(example_probs_1)\n",
    "print(f\"Optimal Coding Scheme for the first dictionary: {coding_scheme_1}\")\n",
    "# Generate a bunch of values from the second dictionary\n",
    "def generate_values(p: Dict[str, float], n: int) -> List[str]:\n",
    "    symbols = list(p.keys())\n",
    "    probabilities = list(p.values())\n",
    "    return list(np.random.choice(symbols, size=n, p=probabilities))\n",
    "\n",
    "generated_values = generate_values(example_probs_2, 1000)\n",
    "print(f\"Generated values from the second dictionary: {generated_values[:50]}\")\n",
    "\n",
    "# Demonstrate the difference in coding schemes\n",
    "def encode_values(values: List[str], coding_scheme: Dict[str, str]) -> List[str]:\n",
    "    return [coding_scheme[value] for value in values]\n",
    "\n",
    "encoded_values_1 = encode_values(generated_values, coding_scheme_1)\n",
    "print(f\"Encoded values using the first dictionary's coding scheme: {encoded_values_1[:50]}\")\n",
    "\n",
    "# Create an optimal coding scheme for the second dictionary (Huffman coding)\n",
    "\n",
    "class HuffmanNode:\n",
    "    def __init__(self, symbol, prob):\n",
    "        self.symbol = symbol\n",
    "        self.prob = prob\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.prob < other.prob\n",
    "\n",
    "def huffman_coding(p: Dict[str, float]) -> Dict[str, str]:\n",
    "    heap = [HuffmanNode(symbol, prob) for symbol, prob in p.items()]\n",
    "    heapq.heapify(heap)\n",
    "    \n",
    "    while len(heap) > 1:\n",
    "        node1 = heapq.heappop(heap)\n",
    "        node2 = heapq.heappop(heap)\n",
    "        merged = HuffmanNode(None, node1.prob + node2.prob)\n",
    "        merged.left = node1\n",
    "        merged.right = node2\n",
    "        heapq.heappush(heap, merged)\n",
    "    \n",
    "    root = heap[0]\n",
    "    coding_scheme = {}\n",
    "    \n",
    "    def generate_codes(node, code):\n",
    "        if node.symbol is not None:\n",
    "            coding_scheme[node.symbol] = code\n",
    "        else:\n",
    "            generate_codes(node.left, code + '0')\n",
    "            generate_codes(node.right, code + '1')\n",
    "    \n",
    "    generate_codes(root, '')\n",
    "    return coding_scheme\n",
    "\n",
    "coding_scheme_2 = huffman_coding(example_probs_2)\n",
    "print(f\"Optimal Coding Scheme for the second dictionary (Huffman): {coding_scheme_2}\")\n",
    "\n",
    "encoded_values_2 = encode_values(generated_values, coding_scheme_2)\n",
    "print(f\"Encoded values using the second dictionary's coding scheme: {encoded_values_2[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
